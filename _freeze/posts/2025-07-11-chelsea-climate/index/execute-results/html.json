{
  "hash": "c260219d8c8d17fdeafd4d99e4d461ef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Obtaining climate data to plot climate spaces for Mediterranean pines with CHELSA and EU-Forest\"\ndescription: \"Download climate data from CHELSEA\"\nauthor:\n  - given: Antonio J.\n    family: Pérez-Luque\n    url: https://ajperezluque.com\n    orcid: 0000-0002-1747-0469\ndate: 07-11-2025\ncategories: [R, climate, CHELSEA] # self-defined categories\ncitation: \n  url: https://ajpelu.github.io/web/posts/2025-17-11-climate-chelsea/ \ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nbibliography: ../references_post.bib\ncsl: ../../assets/ecology.csl\n---\n\n\n\n\n\n## Mapping climate spaces for Mediterranean pines\n\nIn this post, I describe how I extracted monthly climate values from the CHELSA v2.1 dataset for thousands of georeferenced points corresponding to four Mediterranean pine species: *Pinus halepensis*, *P. pinaster*, *P. nigra* and *P. sylvestris*. The ultimate goal is to characterize their **climate niches** across Europe.\n\n## Tree occurrence data\n\nThe first step was retrieving tree occurrences from official sources. I used the shapefiles from the EU-Forest dataset [@Mauri2017], which provides high-resolution distribution data for tree species in Europe.\n\n\n## Climate data\n\nCHELSA (Climatologies at High Resolution for the Earth’s Land Surface Areas) offers climate variables (temperature, precipitation, etc.) at 1 km$^{2}$ spatial resolution [@Karger2017; @chelsa-climatologies-2021]. It's widely used in ecological and biogeographic modeling. There are several products available (different temporal resolutions). In this case, I used CHELSEA V.2.1 for get monthly data from 1980 to 2020. \n\n### How to download CHELSA data? \n\nThere are several approachs: direct download of specific months, or get a list of urls that can be then programatically downloaded using `wget` or via Python or R script. \n\nFor instance, if you want to download directly using command line: \n\n``` bash\nwget https://os.zhdk.cloud.switch.ch/chelsav2/GLOBAL/monthly/tas/CHELSA_tas_01_1980_V.2.1.tif\n```\n\nIn R using you can used `download.file()` from base R or `curl_download()` from `curl` pkg. \n\n### About R packages\n\nAlthough there are some R packages for downloading CHELSA data (`chelsa`, `CHELSAcruts`, `climenv`, `ClimDatDownloadR`), many of them are outdated or broken due to changes in URLs or lack of maintenance. After reviewing several of these, I decided to write a custom script instead.\n\n\n## R Workflow to downloada and extract climate data\n\nInitially, I wrote a script that reads a list of URLs, downloads each raster temporarily, extracts climate values for each point, and then removes the raster to save space.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\nlibrary(sf)\nlibrary(purrr)\nlibrary(dplyr)\n\n# Read occurrence points\nph <- st_read(\"Pinus halepensis.shp\") |> st_cast(\"POINT\")\npn <- st_read(\"Pinus nigra.shp\") |> st_cast(\"POINT\")\nps <- st_read(\"Pinus sylvestris.shp\") |> st_cast(\"POINT\")\npp <- st_read(\"Pinus pinaster.shp\") |> st_cast(\"POINT\")\n\npts <- bind_rows(ph, pn, ps, pp) |>\n  mutate(ID = row_number()) |>\n  select(-Name)\n\n\n# Load CHELSA URLs\nurls <- readLines(\"urls.txt\") |> trimws()\n\n# Loop over files\nresults <- purrr::map(\n  urls,\n  function(url) {\n    file <- basename(url)\n    dest <- file.path(tempdir(), file)\n    download.file(url, dest, mode = \"wb\")\n    r <- terra::rast(dest)\n    val <- extract(r, pts, bind = TRUE) |> as.data.frame()\n    unlink(dest)\n    return(val)\n  }\n)\n\nclimate_pinus <- reduce(results, inner_join, by = \"ID\")\nsaveRDS(climate_pinus, \"monthly_extracted.rds\")\n```\n:::\n\n\n\nAlthough this worked, it was slow because each iteration downloaded a ~800 MB raster.\n\nBut what if I need to extract data for other locations? The script would still work, but not efficiently. Each run would require re-downloading large files, which is time-consuming and redundant. To address this, my next approach was to define an area of interest (AOI), download the climate data once, clip by AOI, store the clipped raster locally, and then perform the data extraction from this subset. At this point, I decided that using data from the Iberian Peninsula would be sufficient for the analyses I had planned—both current and near-future. So, how did I implement this solution?\n\n## Optimized workflow?\n\nAs I said, my attemp was to download every raster and clip using the bounding box of Iberian Peninsula, then stored them locally (~3.45 GB) and use them for repeated extraction runs. This approach was equal in time comsuming the first time (download and clip by AOI), but very fast when I would like to extract climate data within Iberian Peninsula, because the data did not to be downloaded more than once. \n\nHere is the script I used to do that: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bounding box of Iberian Peninsula (wgs84)\nbbox <- ext(-10.0, 4.5, 35.5, 44.5) \n\noptions(nwarnings = 500) # This one to increase the storage of warnings \n\n# custon function \nclipChelsea <- function(url, bbox, out_dir) {\n  \n  # Download the file \n  file_name <- basename(url)\n  dest_file <- file.path(tempdir(), file_name) # donde descargo\n  out_file <- file.path(out_dir, file_name) # donde guardo \n  \n  \n  tryCatch({\n    download.file(url, dest_file, mode = \"wb\", quiet = FALSE)\n    \n    r <- terra::rast(dest_file)\n    r_crop <- crop(r, bbox)\n    \n    writeRaster(r_crop, out_file, overwrite = TRUE)\n    \n    unlink(dest_file)\n    rm(r, r_crop)\n    gc()\n    \n    message(\"Stored at: \", out_file)\n\n  }, error = function(e) {\n    message(\"Error with \", url, \": \", e$message)\n  })\n  \n}\n\n# read urls \nurls <- readLines(\"urls.txt\") |> trimws()  \nurls <- trimws(urls, which = \"right\") # remove blank space at the end of each line \n\npurrr::walk(urls, ~ clipChelsea(.x, bbox, out_dir = \"./data/\")) \n```\n:::\n\n\n\n\nOnce the data from our AOI (Iberian Peninsula) are downloaded, we could extract data for the desired locations. For instance, supose I have a dataset with ~ 1000 points: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This script generates a random 1000 points dataset within Iberina Peninsula \nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(dplyr)\n\n# Get the Iberian Peninsula countries (Spain + Portugal)\nspa_por <- ne_countries(scale = \"medium\", returnclass = \"sf\") |> \n            filter(admin %in% c(\"Spain\", \"Portugal\")) \n\n# Ensure all points within Iberian Peninsula bounding box \nip_bbox <- st_as_sfc(st_bbox(c(\n  xmin = -10.0, xmax = 4.5,\n  ymin = 35.5, ymax = 44.5\n), crs = 4326))\n\nip <- st_intersection(spa_por, ip_bbox)\n\nset.seed(123)\n\n# Generate 1000 random points inside the Iberian Peninsula polygon\npts <- st_sample(ip, size = 1000, type = \"random\")\n\nplot(st_geometry(ip))\nplot(st_geometry(pts), col = \"blue\", pch = 19, cex=.5, add=TRUE)\n```\n\n::: {.cell-output-display}\n![Location of the random points](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nAnd finally, the script that extracts and tidy the climate data was: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles <- list.files(\"./data/\", pattern = \"*.tif\", full.names = TRUE)\n\nextractChelsea <- function(file) {\n  r <- terra::rast(file)\n  val <- terra::extract(r, vect(pts))\n}\n\nchelsea_data <- files |> purrr::map(extractChelsea) \n\nd <- reduce(chelsea_data, inner_join, by = \"ID\") \n\n# Now we rename the variables to extract year, month and variable and pivot to longer, \n# and also express the units in ºC and mm \n\nmonthly_data <- d |>\n  rename_with(~ str_extract(., \"(tas|pr)_[0-9]{2}_[0-9]{4}\"), \n              .cols = starts_with(\"CHELSA_\")) |>\n  pivot_longer(-ID, names_to = c(\"variable\", \"month\", \"year\"), \n               names_sep = \"_\", values_to = \"value\") |>\n  mutate(year = as.numeric(year),\n         month = as.numeric(month)) |> \n  mutate(value = case_when(\n    variable == \"tas\" ~ (value / 10) - 273.15,    # To ºC \n    variable == \"pr\"  ~ value / 100           # mm/100 \n  ))\n```\n:::\n\n\n\nThis approach took:\n\n```\nuser    system  elapsed \n13.923  2.541   29.779 \n\n```\n\nSo although the local storage uses ~3.45 GB, future extractions are much faster (e.g., 1000 points × 978 variables in under 30 seconds).\n\n\n## What's next?\n\nIn upcoming posts, I’ll use these extracted values to visualize and analyze climate spaces and species-specific climate envelopes.\n\nStay tuned!\n\n# References",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}